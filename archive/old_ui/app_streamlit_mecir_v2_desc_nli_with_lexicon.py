# app_streamlit_mecir_v2_desc_nli_with_lexicon.py
# Fake News Detection (MECIR v2)
# - Claim (any language) -> English (NLLB)
# - Fetch evidence from multiple news APIs (title + description/snippet)
# - Rank by relevance (LaBSE, batched)
# - NLI on (title + description) -> claim (TOP-K only)
# - Relevance-gated contradiction (soft)
# - Landmark specificity guard (replica/location) to prevent false SUPPORT
# - Shows descriptions + optional "Consensus context" panel
# - Loads India entity lexicon from lexicon_out/india_entities.txt (generated by your offline script)
#
# Run:
#   streamlit run app_streamlit_mecir_v2_desc_nli_with_lexicon.py
#
# Secrets:
#   .streamlit/secrets.toml
#     NEWSAPI_KEY="..."
#     GNEWS_KEY="..."
#     NEWSDATA_KEY="..."
#     EVENTREGISTRY_KEY="..."

import os
import re
import unicodedata
from typing import Dict, List, Tuple, Optional
from collections import Counter

# ---- HuggingFace download timeouts (must be set before model downloads)
os.environ.setdefault("HF_HUB_READ_TIMEOUT", "180")
os.environ.setdefault("HF_HUB_CONNECT_TIMEOUT", "60")
os.environ.setdefault("TRANSFORMERS_VERBOSITY", "error")
# Optional speed boost (requires `pip install hf_transfer`)
# os.environ.setdefault("HF_HUB_ENABLE_HF_TRANSFER", "1")

import streamlit as st
import torch
import requests
import spacy
from PIL import Image, ImageFile
from langdetect import detect

from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    AutoModelForSequenceClassification,
    CLIPProcessor,
    CLIPModel,
)
from sentence_transformers import SentenceTransformer, util

ImageFile.LOAD_TRUNCATED_IMAGES = True

# ---------------- PAGE SETUP ----------------
st.set_page_config(page_title="Fake News Detection", layout="wide")
st.title("Fake News Detection")

# ---------------- spaCy LOAD ----------------
DEFAULT_SPACY = "en_core_web_sm"
PREFERRED_TRF = "en_core_web_trf"

def load_spacy():
    for name in [PREFERRED_TRF, DEFAULT_SPACY]:
        try:
            return spacy.load(name), name
        except Exception:
            pass
    return None, ""

nlp, spacy_name = load_spacy()
if nlp is None:
    st.error(
        "spaCy model not found.\n\nRun:\n"
        "  python -m spacy download en_core_web_sm\n\n"
        "Optional (better NER):\n"
        "  python -m spacy download en_core_web_trf\n"
    )
    st.stop()

SUPPORTED_LANGS = {"en", "hi", "kn", "te"}

# ---------------- TEXT UTILS ----------------
def normalize_text(s: str) -> str:
    s = (s or "").strip()
    s = unicodedata.normalize("NFKC", s)
    s = re.sub(r"\s+", " ", s)
    return s

def detect_language(text: str) -> str:
    try:
        lang = detect(text)
        return lang if lang in SUPPORTED_LANGS else "en"
    except Exception:
        return "en"

def keyword_set(text: str) -> set:
    text = normalize_text(text).lower()
    text = re.sub(r"[^a-z0-9\s]", " ", text)
    return set(t for t in text.split() if len(t) > 2)

def clamp01(x: float) -> float:
    return max(0.0, min(1.0, x))

def rel_gate(relevance: float, r0: float, r1: float) -> float:
    if r1 <= r0:
        return 1.0
    return clamp01((relevance - r0) / (r1 - r0))

def clean_desc(s: str) -> str:
    s = normalize_text(s or "")
    s = re.sub(r"\[\+\d+\s+chars\]$", "", s).strip()
    return s

def short(s: str, n: int = 260) -> str:
    s = normalize_text(s)
    if len(s) <= n:
        return s
    return s[: n - 3].rstrip() + "..."

# ---------------- INDIA ENTITY LEXICON ----------------
INDIA_ENTITIES_SEED = [
    "Narendra Modi","PM Modi","Prime Minister Modi","Amit Shah","Rahul Gandhi",
    "Virat Kohli","Rohit Sharma","Sachin Tendulkar","MS Dhoni",
    "Mysuru Palace","Mysore Palace","Taj Mahal","India Gate","Red Fort",
    "Charminar","Gateway of India","Golden Temple","Qutub Minar",
    "Ayodhya","Ram Mandir",
    "Mysuru","Mysore","Bengaluru","Bangalore","Mumbai","Delhi","New Delhi",
    "Kolkata","Chennai","Hyderabad","Pune","Ahmedabad","Jaipur","Lucknow",
    "Patna","Bhopal","Indore","Nagpur","Surat","Kanpur","Guwahati",
    "Srinagar","Jammu","Kochi","Thiruvananthapuram",
    "Karnataka","Maharashtra","Tamil Nadu","Telangana","Kerala","Gujarat",
    "Rajasthan","Uttar Pradesh","Bihar","West Bengal","Punjab","Haryana",
    "Madhya Pradesh","Odisha","Assam","Jammu and Kashmir"
]

def load_entities_file(path: str) -> List[str]:
    try:
        if not os.path.exists(path):
            return []
        with open(path, "r", encoding="utf-8") as f:
            lines = [normalize_text(x) for x in f.read().splitlines()]
        lines = [x for x in lines if x]
        seen = set()
        out = []
        for x in lines:
            k = x.lower()
            if k not in seen:
                out.append(x)
                seen.add(k)
        return out
    except Exception:
        return []

# ---------------- CLAIM PARSING ----------------
def extract_spacy_entities(text_en: str) -> List[str]:
    doc = nlp(normalize_text(text_en))
    ents = []
    for ent in doc.ents:
        if ent.label_ in {"PERSON", "ORG", "GPE", "LOC", "EVENT"}:
            ents.append(ent.text.strip())
    seen, out = set(), []
    for e in ents:
        k = e.lower().strip()
        if k and k not in seen:
            out.append(e)
            seen.add(k)
    return out

def extract_lexicon_entities(text_en: str, lex: List[str]) -> List[str]:
    t = " " + normalize_text(text_en).lower() + " "
    found = []
    for name in lex:
        n = name.strip()
        if not n:
            continue
        if re.search(r"\b" + re.escape(n.lower()) + r"\b", t):
            found.append(n)
    if re.search(r"\bmodi\b", t) and "narendra modi" not in [x.lower() for x in found]:
        found.append("Narendra Modi")
    return found

def extract_claim_parts(text_en: str, lex: List[str]):
    text_en = normalize_text(text_en)
    doc = nlp(text_en)

    sp_ents = extract_spacy_entities(text_en)
    lx_ents = extract_lexicon_entities(text_en, lex)

    entities = []
    seen = set()
    for e in sp_ents + lx_ents:
        k = e.lower().strip()
        if k and k not in seen:
            entities.append(e.strip())
            seen.add(k)

    predicate = ""
    for tok in doc:
        if tok.dep_ == "ROOT" and tok.pos_ in {"VERB", "AUX"}:
            predicate = tok.lemma_.lower()
            break

    noun_phrases = []
    try:
        for chunk in doc.noun_chunks:
            t = chunk.text.strip()
            if len(t.split()) >= 2:
                noun_phrases.append(t)
    except Exception:
        pass

    keywords = []
    for tok in doc:
        if tok.is_stop or tok.is_punct:
            continue
        if tok.pos_ in {"NOUN", "PROPN", "VERB"}:
            lemma = tok.lemma_.lower()
            if len(lemma) > 2:
                keywords.append(lemma)

    def dedup(seq):
        seen2, out2 = set(), []
        for x in seq:
            k2 = x.lower().strip()
            if k2 and k2 not in seen2:
                out2.append(x.strip())
                seen2.add(k2)
        return out2

    entities = dedup(entities)
    noun_phrases = dedup(noun_phrases)
    keywords = dedup(keywords)
    phrases = dedup(entities + noun_phrases)[:10]
    return phrases, keywords, entities, predicate

def anchor_tokens_from_claim(claim_en: str, lex: List[str]) -> set:
    anchors = set()
    for ent in extract_spacy_entities(claim_en):
        anchors |= keyword_set(ent)
    for ent in extract_lexicon_entities(claim_en, lex):
        anchors |= keyword_set(ent)
    return anchors

# ---------------- MULTI-QUERY BUILDER ----------------
INDIA_ALIASES = [
    (r"\bmysuru\b", "mysore"),
    (r"\bmysore\b", "mysuru"),
    (r"\bbengaluru\b", "bangalore"),
    (r"\bbangalore\b", "bengaluru"),
    (r"\bnew delhi\b", "delhi"),
    (r"\bdelhi\b", "new delhi"),
    (r"\bpm\b", "prime minister"),
]

def apply_aliases(text: str) -> List[str]:
    outs = [text]
    for pat, rep in INDIA_ALIASES:
        new_outs = []
        for t in outs:
            new_outs.append(re.sub(pat, rep, t, flags=re.IGNORECASE))
        outs = list(dict.fromkeys([normalize_text(x) for x in new_outs if normalize_text(x)]))
    return outs

def build_queries(claim_en: str, lex: List[str]) -> List[str]:
    claim_en = normalize_text(claim_en)
    phrases, keywords, entities, predicate = extract_claim_parts(claim_en, lex)

    queries = []
    top_phrase = phrases[0] if phrases else ""
    top_kw = keywords[:12]

    if entities:
        if predicate:
            queries.append(f"\"{entities[0]}\" {predicate}".strip())
        queries.append(f"\"{entities[0]}\"".strip())

    if top_kw:
        queries.append(" ".join(top_kw[:7]))
    if top_phrase and predicate:
        queries.append(f"\"{top_phrase}\" {predicate}")
    if top_phrase:
        queries.append(f"\"{top_phrase}\"")

    if len(entities) >= 2:
        queries.append(" ".join(entities[:2] + ([predicate] if predicate else [])))

    short_claim = " ".join(claim_en.split()[:12])
    if len(short_claim) >= 8:
        queries.append(short_claim)

    out, seen = [], set()
    for q in queries:
        for v in apply_aliases(normalize_text(q)):
            vn = v.lower().strip()
            if len(vn) < 4:
                continue
            if vn not in seen:
                out.append(v)
                seen.add(vn)
    return out[:10]

# ---------------- SMART FILTER ----------------
def smart_hard_filter(evidence: List[Dict], claim_en: str, lex: List[str]) -> List[Dict]:
    phrases, keywords, entities, _ = extract_claim_parts(claim_en, lex)

    claim_tokens = set(k.lower() for k in keywords)
    for ph in phrases[:5]:
        claim_tokens |= keyword_set(ph)

    entity_tokens = set()
    for e in entities:
        entity_tokens |= keyword_set(e)

    min_overlap = 1 if (len(claim_tokens) <= 6 or len(entity_tokens) >= 2) else 2

    syn = {
        "explode": {"blast", "explosion"},
        "explodes": {"explosion", "blast"},
        "exploded": {"explosion", "blast"},
        "explosion": {"blast", "explodes"},
        "topple": {"collapse", "collapsed", "falls", "fall", "topples"},
        "collapse": {"topple", "topples", "falls", "fall", "crash"},
        "retire": {"retires", "retired", "quit", "resign"},
    }

    def expand(tokens: set) -> set:
        expanded = set(tokens)
        for t in list(tokens):
            if t in syn:
                expanded |= syn[t]
            for k, vs in syn.items():
                if t in vs:
                    expanded.add(k)
                    expanded |= vs
        return expanded

    claim_tokens = expand(claim_tokens | entity_tokens)

    filtered = []
    for art in evidence:
        title = normalize_text(art.get("title", ""))
        desc = normalize_text(art.get("description", ""))
        blob = (title + " " + desc).strip()
        if not blob:
            continue
        tset = keyword_set(blob)
        overlap = len(tset & claim_tokens)
        has_entity = len(tset & entity_tokens) > 0
        if overlap >= min_overlap or has_entity:
            filtered.append(art)
    return filtered

# ---------------- LANDMARK SPECIFICITY GUARD ----------------
QUALIFIER_TOKENS = {"replica","lookalike","reproduction","model","imitation","copy","miniature","mock","theme","park"}
LANDMARK_TERMS = {
    "statue of liberty","eiffel tower","taj mahal","colosseum",
    "big ben","golden gate bridge","white house","buckingham palace"
}

def extract_locations(text: str) -> set:
    doc = nlp(normalize_text(text))
    locs = set()
    for ent in doc.ents:
        if ent.label_ in {"GPE", "LOC"}:
            locs.add(ent.text.lower().strip())
    return locs

def contains_landmark(text: str) -> Optional[str]:
    t = normalize_text(text).lower()
    for lm in LANDMARK_TERMS:
        if lm in t:
            return lm
    return None

def landmark_support_multiplier(claim_en: str, title: str, description: str) -> float:
    claim_l = normalize_text(claim_en).lower()
    if not contains_landmark(claim_l):
        return 1.0

    blob = (normalize_text(title) + " " + normalize_text(description)).lower().strip()
    claim_tokens = keyword_set(claim_l)
    blob_tokens = keyword_set(blob)

    ev_qual = blob_tokens & QUALIFIER_TOKENS
    if ev_qual and len(claim_tokens & ev_qual) == 0:
        return 0.0

    claim_locs = extract_locations(claim_l)
    ev_locs = extract_locations(blob)

    if ev_locs and not claim_locs:
        return 0.0

    if ev_locs and claim_locs and len(ev_locs & claim_locs) == 0:
        return 0.2

    return 1.0

# ---------------- CLAIM VARIANTS (ROBUST NLI) ----------------
def claim_variants_for_nli(claim_en: str) -> List[str]:
    orig = normalize_text(claim_en)
    c = orig.lower()

    variants = set([orig])
    c1 = c
    c1 = re.sub(r"\bexplodes\b", "explosion", c1)
    c1 = re.sub(r"\bexploded\b", "explosion", c1)
    c1 = re.sub(r"\bblast\b", "explosion", c1)
    c1 = re.sub(r"\btopples\b", "collapses", c1)
    c1 = re.sub(r"\btopple\b", "collapse", c1)
    variants.add(normalize_text(c1))
    variants.add(normalize_text(re.sub(r"\bnear\b", "at", c1)))

    out = [normalize_text(v) for v in variants if len(v.split()) >= 3]
    return out[:6]

# ---------------- API KEYS ----------------
def get_key(name: str) -> str:
    if name in st.secrets:
        return str(st.secrets[name])
    return os.getenv(name, "")

NEWSAPI_KEY = get_key("NEWSAPI_KEY")
GNEWS_KEY = get_key("GNEWS_KEY")
NEWSDATA_KEY = get_key("NEWSDATA_KEY")
EVENTREGISTRY_KEY = get_key("EVENTREGISTRY_KEY")

# ---------------- REQUEST HELPERS ----------------
HEADERS = {"User-Agent": "FakeNewsMECIRv2/1.0"}

def safe_get_json(url: str, timeout: int = 20) -> Tuple[dict, str]:
    try:
        r = requests.get(url, timeout=timeout, headers=HEADERS)
        if r.status_code != 200:
            return {}, f"HTTP {r.status_code}: {r.text[:200]}"
        return r.json(), ""
    except Exception as e:
        return {}, str(e)

def safe_post_json(url: str, payload: dict, timeout: int = 25) -> Tuple[dict, str]:
    try:
        r = requests.post(url, json=payload, timeout=timeout, headers=HEADERS)
        if r.status_code != 200:
            return {}, f"HTTP {r.status_code}: {r.text[:200]}"
        return r.json(), ""
    except Exception as e:
        return {}, str(e)

# ---------------- NEWS FETCHERS (title + description) ----------------
def fetch_newsapi_org(query: str) -> Tuple[List[Dict], str]:
    if not NEWSAPI_KEY:
        return [], "Missing NEWSAPI_KEY"
    url = (
        "https://newsapi.org/v2/everything?"
        f"q={requests.utils.quote(query)}&language=en&pageSize=25&sortBy=publishedAt&apiKey={NEWSAPI_KEY}"
    )
    res, err = safe_get_json(url)
    arts = []
    for a in (res.get("articles", []) or []):
        t = a.get("title") or ""
        u = a.get("url") or ""
        d = clean_desc(a.get("description") or a.get("content") or "")
        if t and u:
            arts.append({"title": t, "description": d, "url": u, "api": "NewsAPI.org"})
    return arts, err

def fetch_gnews(query: str) -> Tuple[List[Dict], str]:
    if not GNEWS_KEY:
        return [], "Missing GNEWS_KEY"
    url = f"https://gnews.io/api/v4/search?q={requests.utils.quote(query)}&lang=en&max=25&token={GNEWS_KEY}"
    res, err = safe_get_json(url)
    arts = []
    for a in (res.get("articles", []) or []):
        t = a.get("title") or ""
        u = a.get("url") or ""
        d = clean_desc(a.get("description") or a.get("content") or "")
        if t and u:
            arts.append({"title": t, "description": d, "url": u, "api": "GNews"})
    return arts, err

def fetch_newsdata(query: str) -> Tuple[List[Dict], str]:
    if not NEWSDATA_KEY:
        return [], "Missing NEWSDATA_KEY"
    url = f"https://newsdata.io/api/1/news?q={requests.utils.quote(query)}&language=en&apikey={NEWSDATA_KEY}"
    res, err = safe_get_json(url)
    arts = []
    for a in (res.get("results", []) or []):
        t = a.get("title") or ""
        u = a.get("link") or ""
        d = clean_desc(a.get("description") or a.get("content") or "")
        if t and u:
            arts.append({"title": t, "description": d, "url": u, "api": "NewsData.io"})
    return arts, err

def fetch_eventregistry(query: str) -> Tuple[List[Dict], str]:
    if not EVENTREGISTRY_KEY:
        return [], "Missing EVENTREGISTRY_KEY"
    url = "https://eventregistry.org/api/v1/article/getArticles"
    payload = {
        "action": "getArticles",
        "keyword": query,
        "lang": "eng",
        "articlesPage": 1,
        "articlesCount": 25,
        "articlesSortBy": "date",
        "articlesSortByAsc": False,
        "resultType": "articles",
        "apiKey": EVENTREGISTRY_KEY,
        # some plans ignore these, harmless:
        "includeArticleConcepts": False,
        "includeArticleCategories": False,
        "includeArticleLocation": False,
    }
    res, err = safe_post_json(url, payload)
    results = (((res.get("articles") or {}).get("results")) or [])
    arts = []
    for a in results:
        t = a.get("title") or ""
        u = a.get("url") or ""
        d = clean_desc(a.get("summary") or a.get("body") or a.get("snippet") or "")
        if t and u:
            arts.append({"title": t, "description": d, "url": u, "api": "EventRegistry"})
    return arts, err

def dedup_articles(articles: List[Dict]) -> List[Dict]:
    seen, out = set(), []
    for a in articles:
        t = normalize_text(a.get("title", "")).lower()
        u = (a.get("url") or "").strip().lower()
        if not t:
            continue
        key = (t, u)
        if key in seen:
            continue
        seen.add(key)
        out.append(a)
    return out

def fetch_all_sources(queries: List[str], max_total: int = 200) -> Tuple[List[Dict], Dict[str, str]]:
    all_arts: List[Dict] = []
    errors: Dict[str, str] = {}

    for q in queries:
        a, e = fetch_newsapi_org(q);   all_arts.extend(a); errors.setdefault("NewsAPI.org", e)
        a, e = fetch_gnews(q);         all_arts.extend(a); errors.setdefault("GNews", e)
        a, e = fetch_newsdata(q);      all_arts.extend(a); errors.setdefault("NewsData.io", e)
        a, e = fetch_eventregistry(q); all_arts.extend(a); errors.setdefault("EventRegistry", e)

    all_arts = dedup_articles(all_arts)
    return all_arts[:max_total], errors

# ---------------- LOAD MODELS ----------------
LANG_MAP = {"hi": "hin_Deva", "kn": "kan_Knda", "te": "tel_Telu"}

@st.cache_resource
def load_models():
    device = "cuda" if torch.cuda.is_available() else "cpu"

    clip_name = "openai/clip-vit-base-patch32"
    clip_model = CLIPModel.from_pretrained(clip_name).to(device)
    clip_processor = CLIPProcessor.from_pretrained(clip_name)

    labse_model = SentenceTransformer("sentence-transformers/LaBSE")

    translator_name = "facebook/nllb-200-distilled-600M"
    nllb_tokenizer = AutoTokenizer.from_pretrained(translator_name)
    nllb_model = AutoModelForSeq2SeqLM.from_pretrained(translator_name).to(device)

    nli_name = "facebook/bart-large-mnli"
    nli_tokenizer = AutoTokenizer.from_pretrained(nli_name)
    nli_model = AutoModelForSequenceClassification.from_pretrained(nli_name).to(device)

    return device, clip_model, clip_processor, labse_model, nllb_tokenizer, nllb_model, nli_tokenizer, nli_model

device, clip_model, clip_processor, labse_model, nllb_tokenizer, nllb_model, nli_tokenizer, nli_model = load_models()

# ---------------- TRANSLATION ----------------
def translate_to_english(text: str, lang: str) -> str:
    text = normalize_text(text)
    if not text:
        return ""
    if lang == "en":
        return text
    if lang not in LANG_MAP:
        return text

    nllb_tokenizer.src_lang = LANG_MAP[lang]
    inputs = nllb_tokenizer(text, return_tensors="pt", truncation=True, max_length=256).to(device)
    eng_token_id = nllb_tokenizer.convert_tokens_to_ids("eng_Latn")

    with torch.no_grad():
        translated = nllb_model.generate(
            **inputs,
            forced_bos_token_id=eng_token_id,
            max_length=128,
            num_beams=3,
        )
    return nllb_tokenizer.decode(translated[0], skip_special_tokens=True)

# ---------------- CLIP GATE ----------------
def clip_gate_score(image: Image.Image, text_en: str, a: float, b: float) -> Tuple[float, float]:
    if b <= a:
        b = a + 1e-6
    inputs = clip_processor(text=[text_en], images=image, return_tensors="pt", padding=True).to(device)
    with torch.no_grad():
        outputs = clip_model(**inputs)
        img = outputs.image_embeds
        txt = outputs.text_embeds
        img = img / img.norm(dim=-1, keepdim=True)
        txt = txt / txt.norm(dim=-1, keepdim=True)
        s = (img * txt).sum(dim=-1).item()
    g = (s - a) / (b - a)
    return s, clamp01(g)

# ---------------- NLI ----------------
def nli_probs(premise: str, hypothesis: str) -> Tuple[float, float, float]:
    inputs = nli_tokenizer(premise, hypothesis, return_tensors="pt", truncation=True, max_length=256).to(device)
    with torch.no_grad():
        logits = nli_model(**inputs).logits
        probs = torch.softmax(logits, dim=-1).squeeze(0).tolist()
    # BART MNLI order: contradiction, neutral, entailment
    p_con, p_neu, p_ent = probs[0], probs[1], probs[2]
    return p_ent, p_con, p_neu

# ---------------- RELEVANCE (BATCH) ----------------
def rank_by_relevance(evidence: List[Dict], claim_en: str) -> List[Dict]:
    blobs = []
    for a in evidence:
        t = normalize_text(a.get("title", ""))
        d = normalize_text(a.get("description", ""))
        blob = (t + ". " + d).strip() if d else t
        blobs.append(blob)

    claim_emb = labse_model.encode([claim_en], convert_to_tensor=True, normalize_embeddings=True)
    ev_emb = labse_model.encode(blobs, convert_to_tensor=True, normalize_embeddings=True)
    rels = util.cos_sim(claim_emb, ev_emb).squeeze(0).tolist()

    ranked = []
    for art, rel, blob in zip(evidence, rels, blobs):
        ranked.append({**art, "relevance": float(rel), "blob": blob})
    ranked.sort(key=lambda x: x["relevance"], reverse=True)
    return ranked

# ---------------- AGGREGATION (desc-aware NLI) ----------------
def aggregate_nli_topk(
    evidence_ranked: List[Dict],
    claim_en: str,
    top_k: int,
    min_rel_for_con: float,
    rcon_block: float,
    r0: float,
    r1: float,
):
    top = evidence_ranked[:top_k]
    variants = claim_variants_for_nli(claim_en)

    S_ent, S_con = 0.0, 0.0
    scored = []

    for art in top:
        title = normalize_text(art.get("title", ""))
        desc = normalize_text(art.get("description", ""))
        premise = (title + ". " + desc).strip() if desc else title

        best_f_ent, best_f_con, best_f_neu = 0.0, 0.0, 1.0
        best_h = claim_en
        for h in variants:
            fe, fc, fn = nli_probs(premise, h)
            if fe > best_f_ent:
                best_f_ent, best_f_con, best_f_neu = fe, fc, fn
                best_h = h
        f_ent, f_con, f_neu = best_f_ent, best_f_con, best_f_neu

        # Reverse check uses title only (stability)
        r_ent, r_con, r_neu = nli_probs(claim_en, title)

        support_raw = f_ent if r_con < rcon_block else 0.0
        lm_mult = landmark_support_multiplier(claim_en, title, desc)
        support = support_raw * lm_mult

        weighted_con = f_con * (1.0 - f_ent)
        if art["relevance"] >= min_rel_for_con:
            g_rel = rel_gate(art["relevance"], r0, r1)
            weighted_con_gated = weighted_con * g_rel
        else:
            g_rel = 0.0
            weighted_con_gated = 0.0

        S_ent = max(S_ent, support)
        S_con = max(S_con, weighted_con_gated)

        if f_ent >= f_con and f_ent >= f_neu:
            f_label = "ENTAILS"
        elif f_con >= f_neu:
            f_label = "CONTRADICTS"
        else:
            f_label = "NEUTRAL"

        scored.append({
            **art,
            "premise": premise,
            "f_ent": f_ent, "f_con": f_con, "f_neu": f_neu,
            "r_con": r_con,
            "support": support,
            "landmark_mult": lm_mult,
            "g_rel": g_rel,
            "weighted_con_gated": weighted_con_gated,
            "nli_label": f_label,
            "best_hypothesis": best_h,
        })

    scored.sort(key=lambda x: (x["support"], x["relevance"]), reverse=True)
    return S_ent, S_con, scored

# ---------------- CONSENSUS CONTEXT (safe, non-generative) ----------------
STOPWORDS_LIGHT = {
    "said","says","say","report","reports","reported","today","yesterday","tomorrow",
    "after","before","amid","over","under","from","with","without","into","onto","about",
    "near","left","right","new","latest","breaking","video","photos","watch","live",
    "india","indian"
}

def consensus_context(scored_top: List[Dict], claim_en: str, min_sources: int = 2) -> Dict[str, List[str]]:
    if not scored_top:
        return {"keywords": [], "qualifiers": [], "locations": []}

    claim_tokens = keyword_set(claim_en)
    per_article_tokens = []
    per_article_locs = []

    for art in scored_top:
        blob = (normalize_text(art.get("title","")) + " " + normalize_text(art.get("description",""))).strip()
        toks = keyword_set(blob)
        per_article_tokens.append(toks)
        per_article_locs.append(extract_locations(blob))

    tok_counts = Counter()
    for toks in per_article_tokens:
        for t in toks:
            tok_counts[t] += 1

    loc_counts = Counter()
    for locs in per_article_locs:
        for loc in locs:
            loc_counts[loc] += 1

    qual_counts = {q: tok_counts.get(q, 0) for q in QUALIFIER_TOKENS}

    qualifiers = [q for q, c in qual_counts.items() if c >= min_sources and q not in claim_tokens]
    qualifiers.sort(key=lambda q: qual_counts[q], reverse=True)

    locations = [
        loc for loc, c in loc_counts.items()
        if c >= min_sources and loc not in normalize_text(claim_en).lower()
    ]
    locations.sort(key=lambda loc: loc_counts[loc], reverse=True)

    keywords = []
    for t, c in tok_counts.most_common():
        if c < min_sources:
            continue
        if t in claim_tokens:
            continue
        if t in STOPWORDS_LIGHT:
            continue
        if t in QUALIFIER_TOKENS:
            continue
        if len(t) < 4:
            continue
        keywords.append(t)
        if len(keywords) >= 8:
            break

    return {"keywords": keywords, "qualifiers": qualifiers[:6], "locations": locations[:6]}

# ---------------- SIDEBAR ----------------
st.sidebar.header("Input")
uploaded_image = st.sidebar.file_uploader("Upload image (optional)", type=["jpg", "jpeg", "png"])
news_text = st.sidebar.text_area("Enter headline / claim (any language)")

st.sidebar.header("India entities (lexicon)")
use_entity_file = st.sidebar.checkbox("Use lexicon_out/india_entities.txt", value=True)
extra_entities = st.sidebar.text_area("Add extra entities (one per line)", value="")

st.sidebar.header("Evidence selection")
TOP_K_EVIDENCE = st.sidebar.slider("Top-K evidence (NLI runs on these)", 3, 10, 5)
MIN_REL_FOR_CON = st.sidebar.slider("Min relevance to count contradiction", 0.0, 1.0, 0.35, 0.01)

st.sidebar.subheader("Relevance-gated contradiction")
r0 = st.sidebar.slider("Contradiction gate starts (r0)", 0.0, 1.0, 0.50, 0.01)
r1 = st.sidebar.slider("Contradiction gate full (r1)",   0.0, 1.0, 0.70, 0.01)

st.sidebar.header("CLIP settings")
a = st.sidebar.slider("CLIP low threshold (a)", 0.00, 0.60, 0.20, 0.01)
b = st.sidebar.slider("CLIP high threshold (b)", 0.00, 0.60, 0.35, 0.01)

st.sidebar.header("Decision thresholds")
support_th = st.sidebar.slider("SUPPORTED if support ≥", 0.3, 0.95, 0.55, 0.01)
contradict_th = st.sidebar.slider("CONTRADICTED if contradiction ≥", 0.3, 0.95, 0.75, 0.01)
margin = st.sidebar.slider("Margin (avoid ties)", 0.0, 0.20, 0.05, 0.01)

st.sidebar.header("NLI support rule")
rcon_block = st.sidebar.slider("Block support if reverse-contradiction ≥", 0.0, 1.0, 0.70, 0.01)

st.sidebar.header("UI")
show_consensus = st.sidebar.checkbox("Show consensus context panel", value=True)
show_debug = st.sidebar.checkbox("Show debug", value=False)
items_to_show = st.sidebar.slider("Evidence items to show", 3, 10, 5)

# ---------------- Build entity lexicon ----------------
lex = INDIA_ENTITIES_SEED[:]
if use_entity_file:
    lex += load_entities_file(os.path.join("lexicon_out", "india_entities.txt"))

for line in extra_entities.splitlines():
    line = normalize_text(line)
    if line:
        lex.append(line)

# dedup
seen_lex = set()
lex2 = []
for x in lex:
    k = x.lower()
    if k not in seen_lex:
        lex2.append(x)
        seen_lex.add(k)
lex = lex2

st.sidebar.caption(f"Entities available: {len(lex)}")

# ---------------- DISPLAY IMAGE ----------------
image: Optional[Image.Image] = None
if uploaded_image:
    try:
        image = Image.open(uploaded_image).convert("RGB")
        st.image(image, caption="Uploaded image", use_container_width=True)
    except Exception:
        st.warning("Could not read the uploaded image. Try a different file.")
        image = None

# ---------------- MAIN ----------------
if news_text and len(news_text.strip()) > 0:
    st.subheader("Processing")

    T_raw = normalize_text(news_text)
    lang = detect_language(T_raw)
    T_en = translate_to_english(T_raw, lang)

    st.write(f"spaCy: {spacy_name}")
    st.write(f"Detected language: {lang}")
    st.write(f"Claim (English): {T_en}")

    if image is not None:
        _, g_clip = clip_gate_score(image, T_en, a=a, b=b)
        st.write(f"Image-text alignment (CLIP): {g_clip:.3f}")
    else:
        g_clip = 0.0
        st.write("No image uploaded: image-text alignment skipped.")

    queries = build_queries(T_en, lex)

    with st.spinner("Fetching evidence (title + description)..."):
        evidence, source_errors = fetch_all_sources(queries, max_total=250)

    with st.expander("Source status"):
        st.write({
            "NewsAPI.org": "OK" if NEWSAPI_KEY else "Missing key",
            "GNews": "OK" if GNEWS_KEY else "Missing key",
            "NewsData.io": "OK" if NEWSDATA_KEY else "Missing key",
            "EventRegistry": "OK" if EVENTREGISTRY_KEY else "Missing key",
        })
        st.write({k: v for k, v in source_errors.items() if v})

    if not evidence:
        st.warning("No evidence returned (missing keys, quota, or queries too strict).")
        st.stop()

    st.write(f"Fetched {len(evidence)} unique items (before filtering).")

    evidence_f = smart_hard_filter(evidence, T_en, lex)

    anchors = anchor_tokens_from_claim(T_en, lex)
    if anchors:
        anchored = []
        for a_ in evidence_f:
            blob = (normalize_text(a_.get("title","")) + " " + normalize_text(a_.get("description",""))).strip()
            if len(keyword_set(blob) & anchors) > 0:
                anchored.append(a_)
        if len(anchored) >= max(5, TOP_K_EVIDENCE):
            evidence_f = anchored

    st.write(f"After filtering: {len(evidence_f)} items kept.")
    if not evidence_f:
        st.warning("Filtering removed all evidence. Try rephrasing or add an entity in the sidebar.")
        st.stop()

    with st.spinner("Ranking by relevance..."):
        ranked = rank_by_relevance(evidence_f, T_en)

    with st.spinner("Running NLI on top evidence..."):
        S_ent, S_con, scored_top = aggregate_nli_topk(
            ranked, T_en,
            top_k=TOP_K_EVIDENCE,
            min_rel_for_con=MIN_REL_FOR_CON,
            rcon_block=rcon_block,
            r0=r0, r1=r1
        )

    # decision
    if S_con >= contradict_th and S_con > (S_ent + margin):
        decision = "CONTRADICTED"
    elif S_ent >= support_th and S_ent > (S_con + margin):
        decision = "SUPPORTED"
    else:
        decision = "UNVERIFIED"

    st.subheader("Result")
    c1, c2, c3, c4 = st.columns(4)
    with c1:
        st.metric("Support", f"{S_ent:.3f}")
    with c2:
        st.metric("Contradiction", f"{S_con:.3f}")
    with c3:
        st.metric("Image-text alignment", f"{g_clip:.3f}")
    with c4:
        st.metric("Decision", decision)

    st.subheader("Explanation")
    if decision == "SUPPORTED":
        st.write("- One or more highly relevant items support the claim.")
    elif decision == "CONTRADICTED":
        st.write("- One or more highly relevant items contradict the claim.")
    else:
        st.write("- No item strongly supports the claim, so it is treated as unverified.")

    lm = contains_landmark(T_en)
    if lm:
        st.write(f"- Landmark guard is active for this claim ({lm}). Replica/location specificity is handled.")

    if show_consensus and scored_top:
        ctx = consensus_context(scored_top[:max(3, min(5, len(scored_top)))], T_en, min_sources=2)
        if ctx["qualifiers"] or ctx["locations"] or ctx["keywords"]:
            st.subheader("Consensus context found in evidence")
            if ctx["qualifiers"]:
                st.write("Qualifiers mentioned by multiple sources:", ", ".join(ctx["qualifiers"]))
            if ctx["locations"]:
                st.write("Locations mentioned by multiple sources:", ", ".join(ctx["locations"]))
            if ctx["keywords"]:
                st.write("Other repeated context words:", ", ".join(ctx["keywords"]))
        else:
            st.subheader("Consensus context found in evidence")
            st.write("No consistent extra context across sources (descriptions vary or are missing).")

    st.subheader("Evidence")
    for art in scored_top[:items_to_show]:
        title = normalize_text(art.get("title",""))
        desc = normalize_text(art.get("description",""))
        desc_line = short(desc, 260) if desc else "(No description provided by this API.)"
        st.markdown(
            f"""
**{title}**  
{desc_line}  
Source: {art.get("api","")} | Relevance: {art.get("relevance",0.0):.3f}  
NLI: {art.get("nli_label","")} (ent={art.get("f_ent",0.0):.2f}, con={art.get("f_con",0.0):.2f}, neu={art.get("f_neu",0.0):.2f})  
Support used: {art.get("support",0.0):.2f} (landmark_mult={art.get("landmark_mult",1.0):.2f})  
Contradiction used: {art.get("weighted_con_gated",0.0):.2f} (rel_gate={art.get("g_rel",0.0):.2f})  
[Read article]({art.get("url","")})
---
"""
        )

    if show_debug:
        st.subheader("Debug")
        st.write("Queries:", queries)
        st.write("Anchors:", sorted(list(anchors))[:80])
        st.write("Claim variants:", claim_variants_for_nli(T_en))
        if scored_top:
            st.write("Top premise used (first item):", (scored_top[0].get("premise","") or "")[:700])

st.divider()
st.caption("Decision-support system only. Evidence is headline+description; verify full articles when needed.")
